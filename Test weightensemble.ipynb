{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a9604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weightensemble import WeightForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566fad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as randGen\n",
    "\n",
    "from sklearn import clone\n",
    "\n",
    "# 판별함수\n",
    "def make_label(x):\n",
    "    if x[0] > 0.95:\n",
    "        return 0\n",
    "    result = 1\n",
    "    if x[1] < 0:\n",
    "        result += 1\n",
    "    if (x[2] <= 0.8 and x[3] <= 0.8) or (x[2] > 0.8 and x[3] > 0.8):\n",
    "        result += 1\n",
    "    return result\n",
    "\n",
    "# 정규분포를 따르는 4개의 features를 가지는 X, 위의 판별함수를 사용하여 y, 로 dataset을 만드는 함수\n",
    "def make_dataset(bias, norm_scale, data_size):\n",
    "    x0 = randGen.normal(bias[0], norm_scale, data_size)\n",
    "    x1 = randGen.normal(bias[1], norm_scale, data_size)\n",
    "    x2 = randGen.normal(bias[2], norm_scale, data_size)\n",
    "    x3 = randGen.normal(bias[3], norm_scale, data_size)\n",
    "    X = np.stack((x0, x1, x2, x3), axis=1)\n",
    "    y = np.array(list(map(make_label, X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96ae4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightForestClassifier(punishment=0.01, reward=0.0001)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(None)\n",
    "\n",
    "# 99.9%의 확률로 (-1에서 1사이의 값 + bias)가 나오는 정규분포를 features로 가지는 100,000개의 dataset\n",
    "fitX, fity = make_dataset((0, 0, 0, 0), 1/3, 100000)\n",
    "\n",
    "rfc = clone(RandomForestClassifier(max_depth=None, n_estimators=100))\n",
    "rfc.fit(fitX, fity)\n",
    "\n",
    "# 이름 짓는 방식 : wfc_(log10(reward)+4)_(log10(punishment)+2)_(각 회차마다 훈련시킬 데이터크기)\n",
    "wfc_2_2_100 = clone(WeightForestClassifier(max_depth=None, n_estimators=100))\n",
    "wfc_2_2_100.fit(fitX, fity)\n",
    "\n",
    "wfc_0_0_100 = clone(WeightForestClassifier(max_depth=None, n_estimators=100, reward=0.0001, punishment=0.01))\n",
    "wfc_0_0_100.fit(fitX, fity)\n",
    "\n",
    "wfc_2_2_1 = clone(WeightForestClassifier(max_depth=None, n_estimators=100))\n",
    "wfc_2_2_1.fit(fitX, fity)\n",
    "\n",
    "wfc_0_0_1 = clone(WeightForestClassifier(max_depth=None, n_estimators=100, reward=0.0001, punishment=0.01))\n",
    "wfc_0_0_1.fit(fitX, fity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c245317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc | wfc_2_2_100 | wfc_0_0_100 | wfc_2_2_1 | wfc_0_0_1\n",
      "1.0 | 1.0 | 1.0 | 1.0 | 1.0\n",
      "1.0 | 1.0 | 1.0 | 1.0 | 1.0\n",
      "1.0 | 1.0 | 1.0 | 1.0 | 1.0\n",
      "0.999 | 1.0 | 0.998 | 1.0 | 1.0\n",
      "0.999 | 0.999 | 0.999 | 0.999 | 0.999\n",
      "0.999 | 0.999 | 0.998 | 0.999 | 0.998\n",
      "0.995 | 0.994 | 0.994 | 0.994 | 0.993\n",
      "0.994 | 0.994 | 0.993 | 0.994 | 0.994\n",
      "0.986 | 0.99 | 0.985 | 0.986 | 0.985\n",
      "0.981 | 0.987 | 0.98 | 0.982 | 0.98\n",
      "0.974 | 0.986 | 0.974 | 0.981 | 0.975\n",
      "0.965 | 0.972 | 0.963 | 0.97 | 0.964\n",
      "0.938 | 0.95 | 0.934 | 0.949 | 0.934\n",
      "0.912 | 0.925 | 0.911 | 0.932 | 0.914\n",
      "0.88 | 0.903 | 0.876 | 0.908 | 0.883\n",
      "0.842 | 0.87 | 0.841 | 0.883 | 0.842\n",
      "0.807 | 0.828 | 0.798 | 0.879 | 0.807\n",
      "0.751 | 0.777 | 0.746 | 0.853 | 0.756\n",
      "0.688 | 0.72 | 0.684 | 0.845 | 0.702\n",
      "0.623 | 0.646 | 0.614 | 0.837 | 0.632\n"
     ]
    }
   ],
   "source": [
    "# bias를 0.05씩 늘려가면서 1000개씩의 dataset으로 가중치에 학습없이 score확인\n",
    "print(\"rfc | wfc_2_2_100 | wfc_0_0_100 | wfc_2_2_1 | wfc_0_0_1\")\n",
    "for i in range(20):\n",
    "    bias = i * 0.05\n",
    "    testX, testy = make_dataset((bias, bias, bias, bias), 1/3, 1000)\n",
    "    print(rfc.score(testX, testy), \"|\",\n",
    "          wfc_2_2_100.score(testX, testy), \"|\",\n",
    "          wfc_0_0_100.score(testX, testy), \"|\",\n",
    "          wfc_2_2_1.score(testX, testy), \"|\",\n",
    "          wfc_0_0_1.score(testX, testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f5a22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999 | 0.999 | 0.999 | 0.999 | 0.999\n",
      "1.0 | 1.0 | 1.0 | 1.0 | 1.0\n",
      "0.999 | 1.0 | 1.0 | 0.999 | 1.0\n",
      "0.999 | 0.999 | 0.999 | 1.0 | 1.0\n",
      "0.998 | 0.999 | 0.998 | 0.998 | 0.998\n",
      "0.994 | 0.995 | 0.994 | 0.994 | 0.994\n",
      "0.998 | 0.999 | 0.997 | 0.999 | 0.999\n",
      "0.99 | 0.991 | 0.989 | 0.989 | 0.99\n",
      "0.984 | 0.987 | 0.984 | 0.988 | 0.985\n",
      "0.982 | 0.992 | 0.981 | 0.989 | 0.982\n",
      "0.964 | 0.981 | 0.963 | 0.982 | 0.967\n",
      "0.959 | 0.971 | 0.958 | 0.997 | 0.96\n",
      "0.937 | 0.956 | 0.933 | 0.996 | 0.938\n",
      "0.921 | 0.997 | 0.917 | 0.997 | 0.926\n",
      "0.87 | 0.99 | 0.869 | 0.993 | 0.879\n",
      "0.839 | 0.991 | 0.839 | 0.997 | 0.851\n",
      "0.813 | 0.994 | 0.809 | 0.996 | 0.826\n",
      "0.718 | 0.994 | 0.718 | 0.999 | 0.733\n",
      "0.718 | 0.99 | 0.718 | 1.0 | 0.761\n",
      "0.635 | 0.989 | 0.641 | 0.996 | 0.674\n"
     ]
    }
   ],
   "source": [
    "# 같은 방식으로 가중치를 학습시켜가며 score확인\n",
    "for i in range(20):\n",
    "    bias = i * 0.05\n",
    "    testX, testy = make_dataset((bias, bias, bias, bias), 1/3, 1000)\n",
    "\n",
    "    trainX_100 = testX[:100]\n",
    "    trainy_100 = testy[:100]\n",
    "    wfc_2_2_100.weight_fit(testX, testy)\n",
    "    wfc_0_0_100.weight_fit(testX, testy)\n",
    "    trainX_1 = testX[:1]\n",
    "    trainy_1 = testy[:1]\n",
    "    wfc_2_2_1.weight_fit(testX, testy)\n",
    "    wfc_0_0_1.weight_fit(testX, testy)\n",
    "\n",
    "    print(rfc.score(testX, testy), \"|\",\n",
    "          wfc_2_2_100.score(testX, testy), \"|\",\n",
    "          wfc_0_0_100.score(testX, testy), \"|\",\n",
    "          wfc_2_2_1.score(testX, testy), \"|\",\n",
    "          wfc_0_0_1.score(testX, testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2008c67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1157.43 -1741.21 -2444.17 -1622.03 -1545.27 -1455.38  -746.36 -2442.15\n",
      " -1721.01 -1943.21 -1446.29 -1393.77 -1183.69 -1404.88    90.93 -1397.81\n",
      "   155.57 -1541.23 -1308.93  -638.29 -2702.73 -1107.94 -2496.69 -2331.05\n",
      "  -721.11 -1491.74 -1880.59 -2465.38 -1555.37 -1130.16 -1582.64 -2497.7\n",
      " -1204.9  -2436.09    31.34 -2558.3  -2765.35 -1883.62   124.26  -427.2\n",
      "  -543.35 -1253.38 -1293.78 -2216.92 -1118.04 -1115.01 -2723.94 -1106.93\n",
      " -1643.24 -2077.54   192.94 -1167.53 -1547.29 -2797.67 -2034.11 -1167.53\n",
      " -1501.84 -1569.51 -1103.9  -1745.25  -753.43 -2445.18 -1205.91 -1218.03\n",
      " -1408.92 -1078.65 -1268.53 -1831.1   -527.19 -1264.49 -1586.68 -2436.09\n",
      " -1963.41   206.07 -1221.06 -1406.9     40.43 -2450.23 -1855.34 -1278.63\n",
      " -2459.32 -1217.02 -1134.2  -3028.96  -822.11 -1452.35 -1411.95 -1735.15\n",
      " -2504.77 -1321.05 -1875.54 -2661.32 -2480.53 -2034.11 -1101.88 -2488.61\n",
      "  -630.21 -1177.63 -1058.45 -3096.63]\n",
      "[ 82.6686  84.3553  66.6399  88.1529  87.8398  84.0523  88.062   73.8715\n",
      "  85.6279  74.7603  87.9408  77.4368  75.033   86.6278  84.7694  73.5483\n",
      "  67.3772  73.4473  80.6284  91.799   84.5472  91.7889  86.9712  81.093\n",
      "  87.0722  87.1126  81.3556  74.7906  81.3152  79.3255  94.2634  84.8603\n",
      "  86.9308  74.3361  70.0739  82.4767  74.6189  82.81    79.1437  86.85\n",
      "  84.2442  82.7696  83.7594  84.7492  71.801   86.3551  85.2138  87.9004\n",
      "  68.0438  83.6281  80.4365  91.9808  86.3955  98.4044  79.9012  82.3151\n",
      "  74.4977  85.7592  94.4856 101.1617  75.3259  84.7694  81.295   88.3145\n",
      "  74.8411  75.7804  80.9213  79.7598  80.4668  91.4556  87.1833  70.3971\n",
      "  73.1847  74.5381  80.7597  79.073   74.5785  72.9423  87.6075  75.5784\n",
      "  83.2948  81.7394 101.798   87.9004 101.6162  87.4863  86.8399  88.3246\n",
      "  79.174   77.861   78.8609  98.5963  80.184   74.0129  68.9023  82.9817\n",
      "  67.9832  81.4263  72.2454  98.0509]\n",
      "[-1.26247e+03 -1.38064e+03 -1.10592e+03  7.17400e+01 -2.15430e+03\n",
      " -1.46447e+03 -1.24429e+03 -2.46134e+03  1.33350e+02 -1.69475e+03\n",
      "  2.26270e+02 -2.69869e+03 -3.11178e+03 -1.58466e+03 -3.22591e+03\n",
      " -1.91089e+03 -1.07764e+03 -1.40387e+03 -2.42902e+03 -2.64112e+03\n",
      " -1.43821e+03 -5.77690e+02 -1.76720e+02 -3.00000e+00 -1.13420e+03\n",
      " -1.09885e+03 -2.00381e+03 -8.95840e+02 -5.17090e+02 -2.59062e+03\n",
      " -1.49780e+03 -1.40892e+03 -1.09784e+03 -2.89665e+03 -1.80888e+03\n",
      " -5.49410e+02 -8.69580e+02 -1.90860e+02 -5.23150e+02 -1.26853e+03\n",
      " -1.46346e+03 -1.33822e+03 -7.62520e+02 -2.00381e+03 -2.35125e+03\n",
      " -3.23803e+03 -1.33216e+03 -3.07037e+03 -2.78151e+03 -2.49265e+03\n",
      " -1.79171e+03 -4.95880e+02 -1.31095e+03 -5.43350e+02 -5.38300e+02\n",
      " -1.35640e+03 -4.77700e+02 -1.75535e+03 -5.72640e+02 -2.75828e+03\n",
      " -1.43417e+03 -1.38569e+03 -1.89271e+03 -2.51285e+03 -1.76242e+03\n",
      " -1.44932e+03 -1.36145e+03 -1.33721e+03 -1.38064e+03 -1.92907e+03\n",
      " -1.44225e+03 -1.09784e+03 -1.72707e+03 -2.59062e+03  1.69710e+02\n",
      " -2.30378e+03 -1.19783e+03 -6.24150e+02 -1.27661e+03 -2.29873e+03\n",
      " -4.46390e+02  1.50520e+02 -1.37054e+03 -1.11501e+03 -1.36953e+03\n",
      " -2.93604e+03 -1.07966e+03 -1.35236e+03 -1.19884e+03 -1.97048e+03\n",
      " -2.46740e+03 -1.08370e+03  2.19200e+02 -4.89820e+02  1.08100e+02\n",
      " -1.20995e+03 -2.21900e+01 -1.89978e+03 -1.00088e+03 -1.50487e+03]\n",
      "[ 83.7291  81.2546  87.9913  98.4549  78.7801  74.629   78.9518  86.4662\n",
      "  70.7708  85.0724 100.283   98.4953  86.1834  85.4158  85.739   69.2255\n",
      "  81.1637  71.1344  74.6795  85.1532  99.1821  81.7798  86.749   84.1634\n",
      "  94.3947  82.4868  69.882   83.5574  82.3858  73.0736  69.4679  80.9213\n",
      "  88.6579  92.0414  85.8905  74.2452  94.2432  73.2857  81.5172  74.2957\n",
      "  74.3664  94.5058  84.2038  89.0013  87.7388  74.8108 101.2021  71.8515\n",
      "  81.3859  97.4348  79.6891  97.8893  99.6871  87.1126  74.5078  74.4674\n",
      "  81.1334  67.963   77.1742  74.5381  80.4567  70.1648  97.5762  77.1641\n",
      "  98.0509  73.1948  82.4969  84.9916  69.3164  74.5482  81.5273  98.8286\n",
      "  70.6698 101.3536  80.0527  91.4758  85.3653  74.2654  73.0938  88.1226\n",
      "  81.7798  87.1732  86.9308  85.6279  86.6682  86.4763 100.889   85.537\n",
      "  88.3953  74.6593  80.5375  86.7894  93.9705  72.1747  98.9296  74.1644\n",
      "  74.326   88.0519  85.2744  83.8806]\n"
     ]
    }
   ],
   "source": [
    "print(wfc_2_2_100.estimators_weight_)\n",
    "print(wfc_0_0_100.estimators_weight_)\n",
    "print(wfc_2_2_1.estimators_weight_)\n",
    "print(wfc_0_0_1.estimators_weight_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2cee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
